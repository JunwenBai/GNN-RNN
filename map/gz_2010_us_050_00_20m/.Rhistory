x <- xd
y <- yd
}
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
if (NROW(x) != length(listw$neighbours))
stop("Input data and weights have different dimensions")
n <- NROW(x)
m <- NCOL(x) + ifelse(is.character(fe),length(unique(group)),0) # reflect FE dummies
stopifnot(is.logical(con$small_asy))
if (method != "eigen") {
if (con$small >= n && con$small_asy)
do_asy <- TRUE
else do_asy <- FALSE
}
else do_asy <- TRUE
if (is.null(con$fdHess)) {
con$fdHess <- method != "eigen" && !do_asy
fdHess <- NULL
}
stopifnot(is.logical(con$fdHess))
xcolnames <- colnames(x)
K <- ifelse(xcolnames[1] == "(Intercept)", 2, 1)
K <- K - ifelse(is.character(fe),1,0) # reflect drop of intercept
wy <- lag.listw(listw, y, zero.policy = zero.policy)
if (any(is.na(wy)))
stop("NAs in lagged dependent variable")
if (type != "lag") {
WX <- create_WX(x, listw, zero.policy = zero.policy,
prefix = "lag")
x <- cbind(x, WX)
m <- NCOL(x) + ifelse(is.character(fe),length(unique(group)),0) # reflect FE dummies
rm(WX)
}
x <- x[,-grep("(Intercept)",colnames(x))]
lm.base <- lm(y ~ x - 1)
aliased <- is.na(coefficients(lm.base))
cn <- names(aliased)
names(aliased) <- substr(cn, 2, nchar(cn))
if (any(aliased)) {
nacoef <- which(aliased)
x <- x[, -nacoef]
}
LL_null_lm <- logLik(lm(y ~ 1))
m <- NCOL(x) + ifelse(is.character(fe),length(unique(group)),0) # reflect FE dummies
similar <- FALSE
lm.null <- lm(y ~ x - 1)
logLik_lm.model <- logLik(lm.null)
AIC_lm.model <- AIC(lm.null)
lm.w <- lm.fit(x, wy)
e.null <- lm.null$residuals
e.w <- lm.w$residuals
e.a <- t(e.null) %*% e.null
e.b <- t(e.w) %*% e.null
e.c <- t(e.w) %*% e.w
env <- new.env()
assign("y", y, envir = env)
assign("wy", wy, envir = env)
assign("x", x, envir = env)
assign("n", n, envir = env)
assign("m", m, envir = env)
assign("K", K, envir = env)
assign("e.a", e.a, envir = env)
assign("e.b", e.b, envir = env)
assign("e.c", e.c, envir = env)
assign("family", "SAR", envir = env)
assign("verbose", !quiet, envir = env)
assign("compiled_sse", con$compiled_sse, envir = env)
assign("can.sim", can.sim, envir = env)
assign("listw", listw, envir = env)
assign("similar", FALSE, envir = env)
assign("f_calls", 0L, envir = env)
assign("hf_calls", 0L, envir = env)
timings[["set_up"]] <- proc.time() - .ptime_start
.ptime_start <- proc.time()
if (!quiet)
cat("Jacobian calculated using ")
interval <- jacobianSetup(method, env, con, pre_eig = con$pre_eig,
trs = trs, interval = interval)
assign("interval", interval, envir = env)
nm <- paste(method, "set_up", sep = "_")
timings[[nm]] <- proc.time() - .ptime_start
.ptime_start <- proc.time()
opt <- optimize(sar.lag.mixed.f, interval = interval, maximum = TRUE,
tol = con$tol.opt, env = env)
rho <- opt$maximum
if (isTRUE(all.equal(rho, interval[1])) || isTRUE(all.equal(rho,
interval[2])))
warning("rho on interval bound - results should not be used")
names(rho) <- "rho"
LL <- opt$objective
optres <- opt
nm <- paste(method, "opt", sep = "_")
timings[[nm]] <- proc.time() - .ptime_start
.ptime_start <- proc.time()
lm.lag <- lm((y - rho * wy) ~ x - 1)
r <- residuals(lm.lag)
fit <- y - r
names(r) <- names(fit)
coef.rho <- coefficients(lm.lag)
tarX <- model.matrix(lm.lag)
tary <- model.response(model.frame(lm.lag))
names(coef.rho) <- colnames(x)
SSE <- deviance(lm.lag)
#s2 <- SSE/n
s2 <- SSE/(n-ifelse(is.character(fe),length(unique(group)),0)) # adjust degrees of freedom
timings[["coefs"]] <- proc.time() - .ptime_start
.ptime_start <- proc.time()
assign("first_time", TRUE, envir = env)
if (con$fdHess) {
coefs <- c(rho, coef.rho)
if (con$compiled_sse) {
ptr <- .Call("hess_lag_init", PACKAGE = "spdep")
assign("ptr", ptr, envir = env)
}
fdHess <- getVmatl(coefs, env, s2, trs, tol.solve = tol.solve,
optim = con$optimHess, optimM = con$optimHessMethod)
if (con$compiled_sse) {
.Call("hess_lag_free", get("ptr", envir = env), PACKAGE = "spdep")
}
if (is.null(trs)) {
rownames(fdHess) <- colnames(fdHess) <- c("rho",
colnames(x))
}
else {
rownames(fdHess) <- colnames(fdHess) <- c("sigma2",
"rho", colnames(x))
}
nm <- paste(method, "fdHess", sep = "_")
timings[[nm]] <- proc.time() - .ptime_start
.ptime_start <- proc.time()
}
else fdHess <- FALSE
LMtest <- NULL
varb <- FALSE
ase <- FALSE
if (method != "eigen" && con$fdHess && !do_asy) {
if (is.null(trs)) {
rest.se <- sqrt(diag(fdHess)[-1])
rho.se <- sqrt(fdHess[1, 1])
}
else {
rest.se <- sqrt(diag(fdHess)[-c(1, 2)])
rho.se <- sqrt(fdHess[2, 2])
}
}
else {
rest.se <- NULL
rho.se <- NULL
LMtest <- NULL
ase <- FALSE
varb <- FALSE
tr <- function(A) sum(diag(A))
if (do_asy && method != "eigen")
eigen_setup(env)
eig <- get("eig", envir = env)
O <- (eig/(1 - rho * eig))^2
omega <- sum(O)
if (is.complex(omega))
omega <- Re(omega)
W <- listw2mat(listw)
A <- solve(diag(n) - rho * W)
AW <- A %*% W
zero <- rbind(rep(0, length(coef.rho)))
xtawxb <- s2 * (t(x) %*% AW %*% x %*% coef.rho)
V <- s2 * (s2 * tr(crossprod(AW)) + crossprod(AW %*%
x %*% coef.rho)) + omega * s2^2
inf1 <- rbind(n/2, s2 * tr(AW), t(zero))
inf2 <- rbind(s2 * tr(AW), V, xtawxb)
xtx <- s2 * crossprod(x)
inf3 <- rbind(zero, t(xtawxb), xtx)
inf <- cbind(inf1, inf2, inf3)
varb <- (s2^2) * solve(inf, tol = tol.solve)
rownames(varb) <- colnames(varb) <- c("sigma", "rho",
colnames(x))
rest.se <- sqrt(diag(varb))[-c(1:2)]
rho.se <- sqrt(varb[2, 2])
TW <- (W %*% W) + crossprod(W)
T22 <- sum(diag(TW))
T21A <- sum(diag(TW %*% A))
LMtest <- ((t(r) %*% W %*% r)/s2)^2
LMtest <- LMtest/(T22 - ((T21A^2) * (rho.se^2)))
ase <- TRUE
timings[["eigen_se"]] <- proc.time() - .ptime_start
}
call <- match.call()
if (method == "SE_classic") {
iC <- get("intern_classic", envir = env)
}
else iC <- NULL
ret <- structure(list(type = type, rho = rho, coefficients = coef.rho,
rest.se = rest.se, LL = LL, s2 = s2, SSE = SSE, parameters = (m +
2), logLik_lm.model = logLik_lm.model, AIC_lm.model = AIC_lm.model,
method = method, call = call, residuals = r, opt = optres,
tarX = tarX, tary = tary, y = y, X = x, fitted.values = fit,
se.fit = NULL, similar = similar, ase = ase, rho.se = rho.se,
LMtest = LMtest, resvar = varb, zero.policy = zero.policy,
aliased = aliased, listw_style = listw$style, interval = interval,
fdHess = fdHess, optimHess = con$optimHess, insert = !is.null(trs),
trs = trs, LLNullLlm = LL_null_lm, timings = do.call("rbind",
timings)[, c(1, 3)], f_calls = get("f_calls", envir = env),
hf_calls = get("hf_calls", envir = env), intern_classic = iC),
class = c("sarlm"))
rm(env)
GC <- gc()
if (zero.policy) {
zero.regs <- attr(listw$neighbours, "region.id")[which(card(listw$neighbours) ==
0)]
if (length(zero.regs) > 0L)
attr(ret, "zero.regs") <- zero.regs
}
if (!is.null(na.act))
ret$na.action <- na.act
ret
}
environment(lagsarlm3) <- environment(lagsarlm)
# Spatial Hausman tests for subsets of regressors -----
Hausman.test2 <- function(sem,index,conleyse=FALSE, lonlat=NULL, dist_cutoff=NULL) {
# This is an alternative to Hausman.test() in the spdep package which tests
# ALL coefficients between OLS and the SEM. This allows testing for just a
# subset of coefficients between these two models.
# sem: regression object estimated via GMerrorsar or errorsarlm
# index: index corresponding to the coefficients to test in the beta vector
# conley: logical, should Conley errors be computed under the NULL?
# lonlat= regdata[,c("lon","lat")]
# Step 0: verify that the SEM has the VC
temp <- get(as.character(sem$call[[3]]))
listw.temp <- get(as.character(sem$call[[4]]))
f.temp <- get(as.character(sem$call[[2]]))
if (is.null(sem$invxpx)) {
sem <- GMerrorsar2(f.temp, temp, listw.temp, returnHcov=TRUE)
}
# Step 1: estimate same model with OLS
ols <- lm(formula(sem), data=temp)
# Step 2: compute  VC of OLS estimate
if (conleyse==TRUE) {
# Semi-arametric sptial HAC by Conley
ols$vcov <- conley(ols, lonlat, dist_cutoff=dist_cutoff)[[1]]
sem$vcov <- sem$invxpx
#cbind(sqrt(diag(ols$vcov)), sqrt(diag(sem$vcov)))
} else {
# consistent estimate of the variance of OLS under the SEM DGP
X   <- as.matrix(cbind(1,ols$model[,-1]))
XXi <-  solve(t(X) %*% X, tol=10^-21)
I   <- diag(1,nrow(temp))
rho <- sem$lambda
W <-listw2mat(listw.temp)
W <- Matrix(W, sparse = TRUE) # improves speed >10-fold
sig2<- sem$s2
#inv1 <- I + rho*W + rho^2*W%*%W + rho^3*W%*%W%*%W # approximation of solve(I-rho*W)
#inv2 <- # approximation of solve(I-rho*t(W))
#ols$vcov <- sig2 * XXi %*% t(X) %*% inv1 %*% inv2 %*% X %*% XXi
ols$vcov <- as.matrix(sig2 * XXi %*% t(X) %*% solve(I-rho*W) %*% solve(I-rho*t(W)) %*% X %*% XXi)
sem$vcov <- sem$invxpx
#cbind(coefficients(ols), coefficients(sem)[-length(coefficients(sem))])
#cbind(sqrt(diag(ols$vcov)), sqrt(diag(sem$vcov)))
}
#cbind(sqrt(diag(ols$consist.vcov)), sqrt(diag(ols$vcov[[1]])))
# Step 2: compute test statistic (see Pace and LeSage, Economic Letters, 2008)
b_ols <- coefficients(ols)[index]
b_sem <- coefficients(sem)[index]
v_ols <- ols$vcov[index,index]
v_sem <- sem$vcov[index,index]
shstat<- (t(b_ols-b_sem) %*% solve(v_ols-v_sem, tol=10^-31) %*% (b_ols-b_sem))[1,1]
k <- length(b_ols) # degrees of freedom
pval <- 1-pchisq(shstat,df=k)
# Step 3: prepare to export result of test
# Compare to Hausman.test(sem)
out<- as.data.frame(cbind(shstat,k,pval))
out$V4 <- paste(colnames(X)[index], collapse=" + ")
colnames(out) <- c("Hausman test", "df", "p-value", "Tested variables")
out
}
environment(Hausman.test2) <- environment(Hausman.test)
# Conley Standard Errors for Cross-sectional data ----
conley<-function(object,coords, dist_cutoff=250) {
# syntax: conley(object=ols, coords=regdata[,c("lon","lat")])
# Note, distance should be specified in KILOMETERS
# Load packages
wants <- c("data.table","geosphere","foreign","reshape","parallel")
has   <- wants %in% rownames(installed.packages())
if(any(!has)) install.packages(wants[!has])
sapply(wants, function(i) require(i, character.only=TRUE))
# Internal functions (note:distance is in meters)
iterateObs<-function(y1,e1,X1,fordist,coefficients,cutoff=250*10^3) {
distances<-lapply(1:nrow(X1), function(k) distHaversine(fordist[k,],as.matrix(fordist)))
XeeXhs<-lapply(1:nrow(X1), function(k) (  t(t(X1[k,])) %*% matrix(nrow=1,ncol=nrow(X1),data=e1[k])   * (matrix(nrow=length(coefficients),ncol=1,1)%*% (t(e1) * (distances[[k]]<=cutoff)))) %*% X1)
XeeXhs <- Reduce("+", XeeXhs)
}
# Loop over different cutoffs
Vlist<- lapply(dist_cutoff, function(d) {
print(paste("Conley errors:",d,"km"))
# Preliminary computations
if (class(object)=="lm") { # OLS - linear model
e <- residuals(object) # get residuals
y <- object$model[,1] # get dependent variable
coef <- coefficients(object) #[-1] # remove intercept
XeeX <- matrix(data=0, nrow=length(coef),ncol=length(coef)) # empty matrix
n <- nrow(object$model) # nunmber of observations
X <- model.matrix(object)
#  X <- as.matrix(object$model[,-1,drop=F]) # independent variable matrix
#if (names(coef)[1]=="(Intercept)") {X<- cbind(1,X) ; colnames(X)[1] <-"(Intercept)"}
} else  if (class(object)=="gmsar") { # SEM- GMM
e <- residuals(object) # get residuals
y <- object$lm.target$model[,1] # get dependent variable
coef <- coefficients(object)[-c(length(coefficients(object)))] # remove intercept and sptial autocorrelation parameter
XeeX <- matrix(data=0, nrow=length(coef),ncol=length(coef)) # empty matrix
n <- length(object$residuals) # nunmber of observations
X <- as.matrix(object$lm.target$model[,-1,drop=F]) # independent variable matrix: note this is a "spatially filtered" X matrix, i.e., X-rho*WX
} else {
print("Regression object not supported")
break
}
# Spatial correlation correction
XeeXhs<- iterateObs(y,e,as.matrix(X),as.matrix(coords),coef,cutoff=d*10^3)
XeeX <- XeeXhs # Reduce("+", XeeXhs)
# Compute variance
invXX = solve(t(X)%*%X, tol=10^-40) * n
XeeX_spatial = XeeX / n
V = (invXX %*% XeeX_spatial %*% invXX) / n
V
}) # end of distance cutoff loop
# Prepare to export
names(Vlist) <- dist_cutoff
Vlist
}
# Parametric SE under the SEM DGP -----
semse<-function(object, listw) {
tempdata <- get(as.character(object$call[[3]]))
f.temp <- get(as.character(object$call[[2]]))
#X   <- as.matrix(cbind(1,object$model[,-1]))
X   <- model.matrix(f.temp, tempdata)[,names(coef(object))]
XXi <-  solve(t(X) %*% X, tol=10^-21)
I   <- diag(1,nrow(tempdata))
sem.temp  <- GMerrorsar2(f.temp , tempdata, listw, returnHcov=TRUE)
rho <- sem.temp$lambda
W <-listw2mat(listw)
sig2<- sem.temp$s2
vcov.param <- sig2 * XXi %*% t(X) %*% solve(I-rho*W) %*% solve(I-rho*t(W)) %*% X %*% XXi
}
# End
View(conley)
View(conley)
View(errorsarlm2)
View(Hausman.test2)
View(lagsarlm2)
View(lagsarlm3)
# Packages
rm(list=ls())
wants <- c("parallel","plyr","rgdal", "gtools","spdep","sqldf","tcltk","OIdata","RColorBrewer","mapproj","maptools","data.table","geoR","rgeos","pscl")
has   <- wants %in% rownames(installed.packages())
if(any(!has)) install.packages(wants[!has])
sapply(wants, function(i) require(i, character.only=TRUE))
dir <- list()
dir$root        <- gsub("/Yield trends and countervailing climate effects_R","",getwd())
dir$root        <- gsub("/Yield trends and countervailing climate effects_R","",getwd())
dir$data        <- paste(dir$root,"/data", sep="")
dir$countyshape <- paste(dir$data,"/gz_2010_us_050_00_20m", sep="")
dir$figures     <- paste(dir$root,"/figures", sep="")
dir$tables      <- paste(dir$root,"/tables", sep="")
source("0_functions.R")
fun$balanced<-function(data, ID, TIME, VARS, required=c("all","shared")) {
# borrowed from: http://stackoverflow.com/questions/25671246/r-elegant-way-to-balance-unbalanced-panel-data
if(is.character(ID)) {
ID <- match(ID, names(data))
}
if(is.character(TIME)) {
TIME <- match(TIME, names(data))
}
if(missing(VARS)) {
VARS <- setdiff(1:ncol(data), c(ID,TIME))
} else if (is.character(VARS)) {
VARS <- match(VARS, names(data))
}
required <- match.arg(required)
idf <- do.call(interaction, c(data[, ID, drop=FALSE], drop=TRUE))
timef <- do.call(interaction, c(data[, TIME, drop=FALSE], drop=TRUE))
complete <- complete.cases(data[, VARS])
tbl <- table(idf[complete], timef[complete])
if (required=="all") {
keep <- which(rowSums(tbl==1)==ncol(tbl))
idx <- as.numeric(idf) %in% keep
} else if (required=="shared") {
keep <- which(colSums(tbl==1)==nrow(tbl))
idx <- as.numeric(timef) %in% keep
}
data[idx, ]
}
map <- readOGR(paste(dir$countyshape,"/gz_2010_us_050_00_20m.shp", sep=""), "gz_2010_us_050_00_20m")
map <- readOGR(paste(dir$countyshape,"/gz_2010_us_050_00_20m.shp", sep=""), "gz_2010_us_050_00_20m")
map <- readOGR(paste(dir$countyshape,"/gz_2010_us_050_00_20m.shp", sep=""))
map <- readOGR(paste(dir$countyshape,"/gz_2010_us_050_00_20m.shp", sep=""))
# Title: Plot yield trend analysis for study region
# Authors: Ariel Ortiz-Bobea
# Date: July 2016
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# 1). Preliminary -----
# Packages
rm(list=ls())
wants <- c("parallel","plyr","rgdal", "gtools","spdep","sqldf","tcltk","OIdata","RColorBrewer","mapproj","maptools","data.table","geoR","rgeos","pscl")
has   <- wants %in% rownames(installed.packages())
if(any(!has)) install.packages(wants[!has])
sapply(wants, function(i) require(i, character.only=TRUE))
# Working directories
dir <- list()
dir$root        <- gsub("/Yield trends and countervailing climate effects_R","",getwd())
dir$data        <- paste(dir$root,"/data", sep="")
dir$countyshape <- paste(dir$data,"/gz_2010_us_050_00_20m", sep="")
dir$figures     <- paste(dir$root,"/figures", sep="")
dir$tables      <- paste(dir$root,"/tables", sep="")
# Functions
source("0_functions.R")
fun$balanced<-function(data, ID, TIME, VARS, required=c("all","shared")) {
# borrowed from: http://stackoverflow.com/questions/25671246/r-elegant-way-to-balance-unbalanced-panel-data
if(is.character(ID)) {
ID <- match(ID, names(data))
}
if(is.character(TIME)) {
TIME <- match(TIME, names(data))
}
if(missing(VARS)) {
VARS <- setdiff(1:ncol(data), c(ID,TIME))
} else if (is.character(VARS)) {
VARS <- match(VARS, names(data))
}
required <- match.arg(required)
idf <- do.call(interaction, c(data[, ID, drop=FALSE], drop=TRUE))
timef <- do.call(interaction, c(data[, TIME, drop=FALSE], drop=TRUE))
complete <- complete.cases(data[, VARS])
tbl <- table(idf[complete], timef[complete])
if (required=="all") {
keep <- which(rowSums(tbl==1)==ncol(tbl))
idx <- as.numeric(idf) %in% keep
} else if (required=="shared") {
keep <- which(colSums(tbl==1)==nrow(tbl))
idx <- as.numeric(timef) %in% keep
}
data[idx, ]
}
# Import spatial data
map <- readOGR(paste(dir$countyshape,"/gz_2010_us_050_00_20m.shp", sep=""), "gz_2010_us_050_00_20m")
dir$countyshape
getwd()
map <- readOGR(dir$countyshape, "gz_2010_us_050_00_20m")
dir$countyshape
map <- readOGR(paste0(dir$countyshape,"/gz_2010_us_050_00_20m.shp"), "gz_2010_us_050_00_20m")
map <- readOGR(paste0(dir$countyshape,"/gz_2010_us_050_00_20m"), "gz_2010_us_050_00_20m.shp")
map <- readOGR(paste0(dir$countyshape), "gz_2010_us_050_00_20m")
origwd = getwd()
setwd(dir$countyshape)
mode(dir$countyshape)
class(dir$countyshape)
dir$countyshape
# Title: Plot yield trend analysis for study region
# Authors: Ariel Ortiz-Bobea
# Date: July 2016
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# 1). Preliminary -----
# Packages
rm(list=ls())
wants <- c("parallel","plyr","rgdal", "gtools","spdep","sqldf","tcltk","OIdata","RColorBrewer","mapproj","maptools","data.table","geoR","rgeos","pscl")
has   <- wants %in% rownames(installed.packages())
if(any(!has)) install.packages(wants[!has])
sapply(wants, function(i) require(i, character.only=TRUE))
# Working directories
dir <- list()
dir$root        <- gsub("/Yield trends and countervailing climate effects_R","",getwd())
dir$data        <- paste(dir$root,"/data", sep="")
dir$countyshape <- paste(dir$data,"/gz_2010_us_050_00_20m", sep="")
dir$figures     <- paste(dir$root,"/figures", sep="")
dir$tables      <- paste(dir$root,"/tables", sep="")
?gsub
setwd("C:\Dropbox\climate_yield_trends\JBT\data\gz_2010_us_050_00_20m")
setwd("C:\\Dropbox\\climate_yield_trends\\JBT\\data\\gz_2010_us_050_00_20m")
origwd
origwd
# Title: Plot yield trend analysis for study region
# Authors: Ariel Ortiz-Bobea
# Date: July 2016
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# 1). Preliminary -----
# Packages
rm(list=ls())
wants <- c("parallel","plyr","rgdal", "gtools","spdep","sqldf","tcltk","OIdata","RColorBrewer","mapproj","maptools","data.table","geoR","rgeos","pscl")
has   <- wants %in% rownames(installed.packages())
if(any(!has)) install.packages(wants[!has])
sapply(wants, function(i) require(i, character.only=TRUE))
# Working directories
dir <- list()
dir$root        <- gsub("/Yield trends and countervailing climate effects_R","",getwd())
dir$root        <- gsub("HI","",getwd())
dir$root        <- gsub(getwd())
dir$root        <- paste(getwd())
# Title: Plot yield trend analysis for study region
# Authors: Ariel Ortiz-Bobea
# Date: July 2016
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# 1). Preliminary -----
# Packages
rm(list=ls())
wants <- c("parallel","plyr","rgdal", "gtools","spdep","sqldf","tcltk","OIdata","RColorBrewer","mapproj","maptools","data.table","geoR","rgeos","pscl")
has   <- wants %in% rownames(installed.packages())
if(any(!has)) install.packages(wants[!has])
sapply(wants, function(i) require(i, character.only=TRUE))
# Working directories
dir <- list()
dir$root        <- paste(getwd())
dir <- list()
dir <- list()
dir$root        <- paste(getwd())
dir$data        <- paste(dir$root,"/data", sep="")
dir$root        <- gsub("/Yield trends and countervailing climate effects_R","",getwd())
getwd()
